import cv2
import mediapipe as mp
import numpy as np
import math
from collections import deque
import firebase_admin
from firebase_admin import credentials, db

# Initialize Firebase Admin SDK
cred = credentials.Certificate("C:/firebase/arm1-34234-firebase-adminsdk-iht0p-d14794f2a0.json")
firebase_admin.initialize_app(cred, {
    'databaseURL': 'https://arm1-34234-default-rtdb.asia-southeast1.firebasedatabase.app/'
})

# Reference to the Firebase Realtime Database
ref = db.reference('/hand_movement')

# Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)
mp_drawing = mp.solutions.drawing_utils

# Initialize Webcam
cap = cv2.VideoCapture(0)
cap.set(3, 640)  # Lower resolution for faster processing (width)
cap.set(4, 480)  # Lower resolution for faster processing (height)

# Movement smoothing buffer
position_buffer = deque(maxlen=5)

def smooth_position(buffer):
    """Calculate the smoothed position."""
    if len(buffer) < 2:
        return buffer[-1]
    avg_x = np.mean([pos[0] for pos in buffer])
    avg_y = np.mean([pos[1] for pos in buffer])
    return [int(avg_x), int(avg_y)]

# Initialize R, U, and V variables
R = 0
U = 0
V = 0  # Placeholder for volume percentage

# Frame skipping variable
frame_count = 0

while cap.isOpened():
    frame_count += 1
    success, img = cap.read()
    if not success:
        print("Ignoring empty camera frame.")
        continue

    # Skip every other frame for improved performance
    if frame_count % 2 != 0:
        continue

    # Flip the image horizontally for a mirrored view and convert it to RGB
    img = cv2.flip(img, 1)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Process the frame with MediaPipe
    results = hands.process(img_rgb)
    img = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)  # Convert back to BGR for OpenCV

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            # Draw hand landmarks on the image
            mp_drawing.draw_landmarks(
                img, hand_landmarks, mp_hands.HAND_CONNECTIONS,
                mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),
                mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2),
            )

            # Extract thumb and index fingertip positions
            h, w, _ = img.shape
            thumb = hand_landmarks.landmark[4]
            index = hand_landmarks.landmark[8]

            thumb_x, thumb_y = int(thumb.x * w), int(thumb.y * h)
            index_x, index_y = int(index.x * w), int(index.y * h)

            # Draw circles at thumb and index fingertip
            cv2.circle(img, (thumb_x, thumb_y), 10, (255, 0, 0), cv2.FILLED)
            cv2.circle(img, (index_x, index_y), 10, (255, 0, 0), cv2.FILLED)

            # Draw a line between thumb and index fingertip
            cv2.line(img, (thumb_x, thumb_y), (index_x, index_y), (255, 255, 255), 3)

            # Calculate distance between thumb and index fingertip for volume simulation
            distance = math.hypot(index_x - thumb_x, index_y - thumb_y)

            # Map distance to a 0-100 range for volume percentage
            V = np.interp(distance, [10, 150], [0, 100])

            # Display the volume percentage on the screen
            cv2.putText(img, f'Volume: {int(V)}%', (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

            # Update the position buffer with the current index fingertip position
            current_position = [index_x, index_y]
            position_buffer.append(current_position)
            smoothed_position = smooth_position(position_buffer)

            # Detect movement using smoothed positions
            if len(position_buffer) > 1:
                dx = smoothed_position[0] - position_buffer[-2][0]
                dy = smoothed_position[1] - position_buffer[-2][1]

                # Interpret movement
                movement = "No movement"
                threshold = 10  # Adjust sensitivity

                if abs(dx) > abs(dy) and abs(dx) > threshold:
                    movement = "Moving Right" if dx > 0 else "Moving Left"
                    if dx > 0:  # Moving Right
                        R += 5
                    elif dx < 0:  # Moving Left
                        R -= 5

                elif abs(dy) > threshold:
                    movement = "Moving UP" if dy > 0 else "Moving DOWN"
                    if dy > 0:  # Moving up
                        U += 5
                    elif dy < 0:  # Moving DOWN
                        U -= 5

                # Display the movement on the screen
                cv2.putText(img, movement, (50, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)

            # Print R, U, and V values in the terminal
            print(f"R value: {R}")
            print(f"U value: {U}")
            print(f"V value: {V}")

            # Send the value of R, U, and V to Firebase Realtime Database
            ref.set({
                'R': R,
                'U': U,
                'V': V
            })

    # Display the processed frame
    cv2.imshow("Hand Movement", img)

    # Break loop on 'q' key press
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release resources
cap.release()
cv2.destroyAllWindows()

